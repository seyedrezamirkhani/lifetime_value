{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znEK1MNRXqWY"
   },
   "outputs": [],
   "source": [
    "#@title Copyright 2019 The Lifetime Value Authors.\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oNqWmn530N-"
   },
   "source": [
    "# Churn Prediction for Kaggle Acquire Valued Customer Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKydJ9qF4KVm"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/lifetime_value/blob/master/notebooks/kaggle_acquire_valued_shoppers_challenge/classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/google/lifetime_value/blob/master/notebooks/kaggle_acquire_valued_shoppers_challenge/classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KObdQwyXH2mC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_probability as tfp\n",
    "from typing import Sequence\n",
    "\n",
    "# install and import ltv\n",
    "# !pip install -q git+https://github.com/google/lifetime_value\n",
    "!pip install -q git+https://github.com/seyedrezamirkhani/lifetime_value\n",
    "import lifetime_value as ltv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K41RmAfNXtu_"
   },
   "outputs": [],
   "source": [
    "tfd = tfp.distributions\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzTaK6fFXMWT"
   },
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQVhF3fhNEr2"
   },
   "outputs": [],
   "source": [
    "COMPANY = '104900040'  # @param { isTemplate: true, type: 'string'}\n",
    "LOSS = 'ziln'  # @param { isTemplate: true, type: 'string'} ['bce', 'ziln']\n",
    "# LOSS = 'bce'  # @param { isTemplate: true, type: 'string'} ['bce', 'ziln']\n",
    "# MODEL = 'linear'  # @param { isTemplate: true, type: 'string'} ['linear', 'dnn']\n",
    "MODEL = 'dnn'  # @param { isTemplate: true, type: 'string'} ['linear', 'dnn']\n",
    "LEARNING_RATE = 0.0002  # @param { isTemplate: true}\n",
    "EPOCHS = 400  # @param {type: 'integer'}\n",
    "DATA_FOLDER = './tmp/acquire-valued-shoppers-challenge' # @param { isTemplate: true, type: 'string'}\n",
    "OUTPUT_CSV_FOLDER = f'{DATA_FOLDER}/result'  # @param { isTemplate: true, type: 'string'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7dg8TwYbxnl"
   },
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURES = ['chain', 'dept', 'category', 'brand', 'productmeasure']\n",
    "NUMERIC_FEATURES = ['log_calibration_value']\n",
    "\n",
    "ALL_FEATURES = CATEGORICAL_FEATURES + NUMERIC_FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_nbvZjMuj_z"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFi0JMPu138h"
   },
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krXMbrkVNtdN"
   },
   "source": [
    "Setup kaggle API correctly following https://www.kaggle.com/docs/api\n",
    "```\n",
    "%%shell\n",
    "mkdir ~/.kaggle\n",
    "echo \\{\\\"username\\\":\\\"{your kaggle username}\\\",\\\"key\\\":\\\"{your kaggle api key}\\\"\\} > ~/.kaggle/kaggle.json\n",
    "pip install kaggle\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set it DATA_FOLDER as an environment variable\n",
    "%env DATA_FOLDER=$DATA_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gf4ipd-14x0"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ -e $DATA_FOLDER/transactions.csv ]\n",
    "then\n",
    "  echo \"File already exists, no need to download.\"\n",
    "else\n",
    "  rm -rf $DATA_FOLDER\n",
    "  mkdir -p $DATA_FOLDER\n",
    "  cd $DATA_FOLDER\n",
    "  kaggle competitions download -c acquire-valued-shoppers-challenge\n",
    "  echo \"Unzip file. This may take 10 min.\"\n",
    "  unzip acquire-valued-shoppers-challenge.zip transactions.csv.gz\n",
    "  gunzip transactions.csv.gz\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4zoAS25uj_7"
   },
   "source": [
    "### Load transaction csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tIMvE3dW1Ky"
   },
   "outputs": [],
   "source": [
    "def load_transaction_data(company):\n",
    "  all_data_filename = f'{DATA_FOLDER}/transactions.csv'\n",
    "  one_company_data_filename = f'{DATA_FOLDER}/transactions_company_{company}.csv'\n",
    "  if os.path.isfile(one_company_data_filename):\n",
    "    df = pd.read_csv(one_company_data_filename)\n",
    "  else:\n",
    "    data_list = []\n",
    "    chunksize = 10**6\n",
    "    # 350 iterations\n",
    "    for chunk in tqdm.tqdm(pd.read_csv(all_data_filename, chunksize=chunksize)):\n",
    "      data_list.append(chunk.query(\"company=={}\".format(company)))\n",
    "    df = pd.concat(data_list, axis=0)\n",
    "    df.to_csv(one_company_data_filename, index=None)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ra4bfwCVwKn"
   },
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlJl5g9Delmi"
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "  df = df.query('purchaseamount>0')\n",
    "  df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "  df['start_date'] = df.groupby('id')['date'].transform('min')\n",
    "\n",
    "  # Compute calibration values\n",
    "  calibration_value = (\n",
    "      df.query('date==start_date').groupby('id')\n",
    "      ['purchaseamount'].sum().reset_index())\n",
    "  calibration_value.columns = ['id', 'calibration_value']\n",
    "\n",
    "  # Compute holdout values\n",
    "  one_year_holdout_window_mask = (\n",
    "      (df['date'] > df['start_date']) &\n",
    "      (df['date'] <= df['start_date'] + np.timedelta64(365, 'D')))\n",
    "  holdout_value = (\n",
    "      df[one_year_holdout_window_mask].groupby('id')\n",
    "      ['purchaseamount'].sum().reset_index())\n",
    "  holdout_value.columns = ['id', 'holdout_value']\n",
    "\n",
    "  # Compute calibration attributes\n",
    "  calibration_attributes = (\n",
    "      df.query('date==start_date').sort_values(\n",
    "          'purchaseamount', ascending=False).groupby('id')[[\n",
    "              'chain', 'dept', 'category', 'brand', 'productmeasure'\n",
    "          ]].first().reset_index())\n",
    "\n",
    "  # Merge dataframes\n",
    "  customer_level_data = (\n",
    "      calibration_value.merge(calibration_attributes, how='left',\n",
    "                              on='id').merge(\n",
    "                                  holdout_value, how='left', on='id'))\n",
    "  customer_level_data['holdout_value'] = (\n",
    "      customer_level_data['holdout_value'].fillna(0.))\n",
    "  customer_level_data[CATEGORICAL_FEATURES] = (\n",
    "      customer_level_data[CATEGORICAL_FEATURES].fillna('UNKNOWN'))\n",
    "\n",
    "  # Specify data types\n",
    "  customer_level_data['log_calibration_value'] = (\n",
    "      np.log(customer_level_data['calibration_value']).astype('float32'))\n",
    "  customer_level_data['chain'] = (\n",
    "      customer_level_data['chain'].astype('category'))\n",
    "  customer_level_data['dept'] = (customer_level_data['dept'].astype('category'))\n",
    "  customer_level_data['brand'] = (\n",
    "      customer_level_data['brand'].astype('category'))\n",
    "  customer_level_data['category'] = (\n",
    "      customer_level_data['category'].astype('category'))\n",
    "  customer_level_data['label'] = (\n",
    "      customer_level_data['holdout_value'].astype('float32'))\n",
    "  return customer_level_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fP3q6uuMoXhA"
   },
   "source": [
    "### Load customer-level csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8B4zV1xoeMX"
   },
   "outputs": [],
   "source": [
    "def load_customer_level_csv(company):\n",
    "  customer_level_data_file = f'{DATA_FOLDER}/customer_level_data_company_{company}.csv'\n",
    "  if os.path.isfile(customer_level_data_file):\n",
    "    customer_level_data = pd.read_csv(customer_level_data_file)\n",
    "  else:\n",
    "    customer_level_data = preprocess(load_transaction_data(company))\n",
    "  for cat_col in CATEGORICAL_FEATURES:\n",
    "    customer_level_data[cat_col] = (\n",
    "        customer_level_data[cat_col].astype('category'))\n",
    "  for num_col in [\n",
    "      'log_calibration_value', 'calibration_value', 'holdout_value'\n",
    "  ]:\n",
    "    customer_level_data[num_col] = (\n",
    "        customer_level_data[num_col].astype('float32'))\n",
    "\n",
    "  return customer_level_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWwMxpIEukAE"
   },
   "outputs": [],
   "source": [
    "# Processes data. 350 iteration in total. May take 10min.\n",
    "customer_level_data = load_customer_level_csv(COMPANY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09tqgvANtsil"
   },
   "source": [
    "We observe a mixture of zero and lognormal distribution of holdout value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtF0z3VbmGev"
   },
   "outputs": [],
   "source": [
    "customer_level_data.label.apply(np.log1p).hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLpgjEuofbdy"
   },
   "outputs": [],
   "source": [
    "customer_level_data.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slIDJAaTcQeK"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4kN0uk4kZ68"
   },
   "source": [
    "### Make train/eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjP5v6NiQfCX"
   },
   "outputs": [],
   "source": [
    "def linear_split(df):\n",
    "  # get_dummies preserves numeric features.\n",
    "  x = pd.get_dummies(df[ALL_FEATURES], drop_first=True).astype('float32').values\n",
    "  y = df['label'].values\n",
    "\n",
    "  x_train, x_eval, y_train, y_eval = model_selection.train_test_split(\n",
    "      x, y, test_size=0.2, random_state=123)\n",
    "\n",
    "  return x_train, x_eval, y_train, y_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjP5v6NiQfCX"
   },
   "outputs": [],
   "source": [
    "def linear_split_with_test(df):\n",
    "  # get_dummies preserves numeric features.\n",
    "  x = pd.get_dummies(df[ALL_FEATURES], drop_first=True).astype('float32').values\n",
    "  y = df['label'].values\n",
    "\n",
    "  x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
    "      x, y, test_size=0.2, random_state=123)\n",
    "\n",
    "  x_train, x_eval, y_train, y_eval = model_selection.train_test_split(\n",
    "      x_train, y_train, test_size=0.2, random_state=123)\n",
    "\n",
    "  return x_train, x_eval, x_test, y_train, y_eval, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtWXwORJjaP4"
   },
   "outputs": [],
   "source": [
    "def dnn_split(df):\n",
    "  for key in CATEGORICAL_FEATURES:\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    df[key] = encoder.fit_transform(df[key])\n",
    "\n",
    "  df_train, df_eval = model_selection.train_test_split(\n",
    "      df, test_size=0.2, random_state=123)\n",
    "\n",
    "  def feature_dict(df):\n",
    "    features = {k: v.values for k, v in dict(df[CATEGORICAL_FEATURES]).items()}\n",
    "    features['numeric'] = df[NUMERIC_FEATURES].values\n",
    "    return features\n",
    "\n",
    "  x_train, y_train = feature_dict(df_train), df_train['label'].values\n",
    "  x_eval, y_eval = feature_dict(df_eval), df_eval['label'].values\n",
    "\n",
    "  return x_train, x_eval, y_train, y_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtWXwORJjaP4"
   },
   "outputs": [],
   "source": [
    "def dnn_split_with_test(df):\n",
    "  for key in CATEGORICAL_FEATURES:\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    df[key] = encoder.fit_transform(df[key])\n",
    "\n",
    "  df_train, df_test = model_selection.train_test_split(      \n",
    "      df, test_size=0.2, random_state=123)\n",
    "\n",
    "  df_train, df_eval = model_selection.train_test_split(\n",
    "      df_train, test_size=0.2, random_state=123)\n",
    "    \n",
    "  def feature_dict(df):\n",
    "    features = {k: v.values for k, v in dict(df[CATEGORICAL_FEATURES]).items()}\n",
    "    features['numeric'] = df[NUMERIC_FEATURES].values\n",
    "    return features\n",
    "\n",
    "  x_train, y_train = feature_dict(df_train), df_train['label'].values\n",
    "  x_eval, y_eval = feature_dict(df_eval), df_eval['label'].values\n",
    "  x_test, y_test = feature_dict(df_test), df_test['label'].values\n",
    "\n",
    "  return x_train, x_eval, x_test, y_train, y_eval, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqbShWBzR4NE"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yh4Y4a89ooP3"
   },
   "outputs": [],
   "source": [
    "def linear_model(output_units, input_dim):\n",
    "  return tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(input_dim,)),\n",
    "    tf.keras.layers.Dense(output_units, activation=None)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8yo3HLtrAE_"
   },
   "outputs": [],
   "source": [
    "def embedding_dim(x):\n",
    "  return int(x**.25) + 1\n",
    "\n",
    "\n",
    "def embedding_layer(vocab_size):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Embedding(\n",
    "          input_dim=vocab_size,\n",
    "          output_dim=embedding_dim(vocab_size)\n",
    "      ),\n",
    "      tf.keras.layers.Flatten(),\n",
    "  ])\n",
    "\n",
    "\n",
    "def dnn_model(output_units, df):\n",
    "  numeric_input = tf.keras.layers.Input(\n",
    "      shape=(len(NUMERIC_FEATURES),), name='numeric')\n",
    "\n",
    "  embedding_inputs = [\n",
    "      tf.keras.layers.Input(shape=(1,), name=key, dtype=np.int64)\n",
    "      for key in CATEGORICAL_FEATURES\n",
    "  ]\n",
    "\n",
    "  embedding_outputs = [\n",
    "      embedding_layer(vocab_size=df[key].nunique())(input)\n",
    "      for key, input in zip(CATEGORICAL_FEATURES, embedding_inputs)\n",
    "  ]\n",
    "\n",
    "  deep_input = tf.keras.layers.concatenate([numeric_input] + embedding_outputs)\n",
    "  deep_model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(32, activation='relu'),\n",
    "      tf.keras.layers.Dense(output_units),\n",
    "  ])\n",
    "  return tf.keras.Model(\n",
    "      inputs=[numeric_input] + embedding_inputs, outputs=deep_model(deep_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8l-KzZ12fbK"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45BHY6q7rQmI"
   },
   "outputs": [],
   "source": [
    "if LOSS == 'bce':\n",
    "  loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "  output_units = 1\n",
    "\n",
    "if LOSS == 'ziln':\n",
    "  loss = ltv.zero_inflated_lognormal_loss\n",
    "  output_units = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Jeou8bGrhll"
   },
   "outputs": [],
   "source": [
    "if MODEL == 'linear':\n",
    "  # x_train, x_eval, y_train, y_eval = linear_split(customer_level_data)\n",
    "  x_train, x_eval, x_test, y_train, y_eval, y_test = linear_split_with_test(customer_level_data)\n",
    "  model = linear_model(output_units, x_train.shape[1])\n",
    "\n",
    "if MODEL == 'dnn':\n",
    "  #x_train, x_eval, y_train, y_eval = dnn_split(customer_level_data)\n",
    "  x_train, x_eval, x_test, y_train, y_eval, y_test = dnn_split_with_test(customer_level_data)\n",
    "  model = dnn_model(output_units, customer_level_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uF2IdTpAwiZV"
   },
   "outputs": [],
   "source": [
    "if LOSS == 'bce':\n",
    "  y_train = (y_train > 0).astype('float32')\n",
    "  y_eval = (y_eval > 0).astype('float32')\n",
    "  y_test = (y_test > 0).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GQ-RlIAfT62"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=loss, optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chEIOzq6rlJx"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', min_lr=1e-6),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BjnHV7MWhK1"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train[:, np.newaxis],\n",
    "    batch_size=1024,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(x_eval, y_eval[:, np.newaxis])).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAJGs5SebDeN"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history)[['loss', 'val_loss']][2:].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHaiutmy2aYm"
   },
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6E_5gYAYQMw"
   },
   "outputs": [],
   "source": [
    "logits = model.predict(x=x_test, batch_size=1024)\n",
    "y_pred = K.sigmoid(logits[..., :1]).numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ICTDpg4rxdlj"
   },
   "outputs": [],
   "source": [
    "y_true = (y_test > 0).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POeY1gdKTwfx"
   },
   "outputs": [],
   "source": [
    "def classification_report(y_true: Sequence[int],\n",
    "                          y_pred: Sequence[float]) -> pd.DataFrame:\n",
    "  \"\"\"Report individual level classification metrics.\n",
    "\n",
    "  Arguments:\n",
    "    y_true: true binary labels.\n",
    "    y_pred: predicted binary labels.\n",
    "\n",
    "  Returns:\n",
    "    out: dataframe with classification metrics as columns.\n",
    "  \"\"\"\n",
    "  out = pd.DataFrame(index=[0])\n",
    "\n",
    "  out['AUC'] = metrics.roc_auc_score(y_true, y_pred)\n",
    "  out['PR_AUC'] = metrics.average_precision_score(y_true, y_pred)\n",
    "  out['precision'] = metrics.precision_score(y_true, 1 * (y_pred > .5))\n",
    "  out['recall'] = metrics.recall_score(y_true, 1 * (y_pred > .5))\n",
    "  out['f1'] = metrics.f1_score(y_true, 1 * (y_pred > .5))\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGcWU2vFaeT1"
   },
   "outputs": [],
   "source": [
    "classification = classification_report(y_true, y_pred)\n",
    "classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-i_AbqhXcurk"
   },
   "source": [
    "### All metrics together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Umqg1-0Bc1HS"
   },
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame(\n",
    "    {\n",
    "        'company': COMPANY,\n",
    "        'model': MODEL,\n",
    "        'loss': LOSS,\n",
    "        'label_mean': y_true.mean(),\n",
    "        'pred_mean': y_pred.mean(),\n",
    "        'AUC': classification.loc[0, 'AUC'],\n",
    "        'PR_AUC': classification.loc[0, 'PR_AUC'],\n",
    "        'precision': classification.loc[0, 'precision'],\n",
    "        'recall': classification.loc[0, 'recall'],\n",
    "        'f1': classification.loc[0, 'f1']\n",
    "    },\n",
    "    index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LV1Hs3xcxnd"
   },
   "outputs": [],
   "source": [
    "df_metrics[[\n",
    "    'company',\n",
    "    'model',\n",
    "    'loss',\n",
    "    'label_mean',\n",
    "    'pred_mean',\n",
    "    'AUC',\n",
    "    'PR_AUC',\n",
    "    'precision',\n",
    "    'recall',\n",
    "    'f1',\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVy6lYn4mSrj"
   },
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtkQ4mqUEFsb"
   },
   "outputs": [],
   "source": [
    "output_path = os.path.join(OUTPUT_CSV_FOLDER, COMPANY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qmLzJqOEFsm"
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(output_path):\n",
    "  os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61B5Zc_UEFsr"
   },
   "outputs": [],
   "source": [
    "output_file = os.path.join(output_path,\n",
    "                           '{}_classification_{}.csv'.format(MODEL, LOSS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqglbXfwEFsv"
   },
   "outputs": [],
   "source": [
    "df_metrics.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "",
    "kind": "local"
   },
   "name": "classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

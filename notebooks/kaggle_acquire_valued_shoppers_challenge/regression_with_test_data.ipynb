{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5RoRxBv3bRjy"
   },
   "outputs": [],
   "source": [
    "#@title Copyright 2019 The Lifetime Value Authors.\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tkQUXmWhqRY"
   },
   "source": [
    "# Lifetime Value prediction for Kaggle Acquire Valued Customer Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pw8bm9nV6YJ5"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/lifetime_value/blob/master/notebooks/kaggle_acquire_valued_shoppers_challenge/regression.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/google/lifetime_value/blob/master/notebooks/kaggle_acquire_valued_shoppers_challenge/regression.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KObdQwyXH2mC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_probability as tfp\n",
    "import tqdm\n",
    "from typing import Sequence\n",
    "\n",
    "# install and import ltv\n",
    "#!pip install -q git+https://github.com/google/lifetime_value\n",
    "!pip install -q git+https://github.com/seyedrezamirkhani/lifetime_value\n",
    "import lifetime_value as ltv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K41RmAfNXtu_"
   },
   "outputs": [],
   "source": [
    "tfd = tfp.distributions\n",
    "%config InlineBackend.figure_format='retina'\n",
    "sns.set_style('whitegrid')\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoN-PRvNuIti"
   },
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GGpDbxd3S5L"
   },
   "outputs": [],
   "source": [
    "COMPANY = '103600030'  # @param { isTemplate: true, type: 'string'}\n",
    "LOSS = 'ziln'  # @param { isTemplate: true, type: 'string'} ['mse', 'ziln']\n",
    "#LOSS = 'mse'  # @param { isTemplate: true, type: 'string'} ['mse', 'ziln']\n",
    "MODEL = 'dnn'  # @param { isTemplate: true, type: 'string'} ['linear', 'dnn']\n",
    "# MODEL = 'linear'  # @param { isTemplate: true, type: 'string'} ['linear', 'dnn']\n",
    "LEARNING_RATE = 0.0002  # @param { isTemplate: true}\n",
    "EPOCHS = 400  # @param { isTemplate: true, type: 'integer'}\n",
    "DATA_FOLDER = './tmp/acquire-valued-shoppers-challenge' # @param { isTemplate: true, type: 'string'}\n",
    "OUTPUT_CSV_FOLDER = f'{DATA_FOLDER}/result'  # @param { isTemplate: true, type: 'string'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UK9Y5NoMtm3X"
   },
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURES = ['chain', 'dept', 'category', 'brand', 'productmeasure']\n",
    "NUMERIC_FEATURES = ['log_calibration_value']\n",
    "\n",
    "ALL_FEATURES = CATEGORICAL_FEATURES + NUMERIC_FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzTaK6fFXMWT"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFi0JMPu138h"
   },
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krXMbrkVNtdN"
   },
   "source": [
    "Setup kaggle API correctly following https://www.kaggle.com/docs/api\n",
    "```\n",
    "%%shell\n",
    "mkdir ~/.kaggle\n",
    "echo \\{\\\"username\\\":\\\"{your kaggle username}\\\",\\\"key\\\":\\\"{your kaggle api key}\\\"\\} > ~/.kaggle/kaggle.json\n",
    "pip install kaggle\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set it DATA_FOLDER as an environment variable\n",
    "%env DATA_FOLDER=$DATA_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gf4ipd-14x0"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ -e $DATA_FOLDER/transactions.csv ]\n",
    "then\n",
    "  echo \"File already exists, no need to download.\"\n",
    "else\n",
    "  rm -rf $DATA_FOLDER\n",
    "  mkdir -p $DATA_FOLDER\n",
    "  cd $DATA_FOLDER\n",
    "  kaggle competitions download -c acquire-valued-shoppers-challenge\n",
    "  echo \"Unzip file. This may take 10 min.\"\n",
    "  unzip acquire-valued-shoppers-challenge.zip transactions.csv.gz\n",
    "  gunzip transactions.csv.gz\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IT53azGsa2a2"
   },
   "source": [
    "### Load transaction csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tIMvE3dW1Ky"
   },
   "outputs": [],
   "source": [
    "def load_transaction_data(company):\n",
    "  all_data_filename = f'{DATA_FOLDER}/transactions.csv'\n",
    "  one_company_data_filename = f'{DATA_FOLDER}/transactions_company_{company}.csv'\n",
    "  if os.path.isfile(one_company_data_filename):\n",
    "    df = pd.read_csv(one_company_data_filename)\n",
    "  else:\n",
    "    data_list = []\n",
    "    chunksize = 10**6\n",
    "    # 350 iterations\n",
    "    for chunk in tqdm.tqdm(pd.read_csv(all_data_filename, chunksize=chunksize)):\n",
    "      data_list.append(chunk.query(\"company=={}\".format(company)))\n",
    "    df = pd.concat(data_list, axis=0)\n",
    "    df.to_csv(one_company_data_filename, index=None)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ra4bfwCVwKn"
   },
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlJl5g9Delmi"
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "  df = df.query('purchaseamount>0')\n",
    "  df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "  df['start_date'] = df.groupby('id')['date'].transform('min')\n",
    "\n",
    "  # Compute calibration values\n",
    "  calibration_value = (\n",
    "      df.query('date==start_date').groupby('id')\n",
    "      ['purchaseamount'].sum().reset_index())\n",
    "  calibration_value.columns = ['id', 'calibration_value']\n",
    "\n",
    "  # Compute holdout values\n",
    "  one_year_holdout_window_mask = (\n",
    "      (df['date'] > df['start_date']) &\n",
    "      (df['date'] <= df['start_date'] + np.timedelta64(365, 'D')))\n",
    "  holdout_value = (\n",
    "      df[one_year_holdout_window_mask].groupby('id')\n",
    "      ['purchaseamount'].sum().reset_index())\n",
    "  holdout_value.columns = ['id', 'holdout_value']\n",
    "\n",
    "  # Compute calibration attributes\n",
    "  calibration_attributes = (\n",
    "      df.query('date==start_date').sort_values(\n",
    "          'purchaseamount', ascending=False).groupby('id')[[\n",
    "              'chain', 'dept', 'category', 'brand', 'productmeasure'\n",
    "          ]].first().reset_index())\n",
    "\n",
    "  # Merge dataframes\n",
    "  customer_level_data = (\n",
    "      calibration_value.merge(calibration_attributes, how='left',\n",
    "                              on='id').merge(\n",
    "                                  holdout_value, how='left', on='id'))\n",
    "  customer_level_data['holdout_value'] = (\n",
    "      customer_level_data['holdout_value'].fillna(0.))\n",
    "  customer_level_data[CATEGORICAL_FEATURES] = (\n",
    "      customer_level_data[CATEGORICAL_FEATURES].fillna('UNKNOWN'))\n",
    "\n",
    "  # Specify data types\n",
    "  customer_level_data['log_calibration_value'] = (\n",
    "      np.log(customer_level_data['calibration_value']).astype('float32'))\n",
    "  customer_level_data['chain'] = (\n",
    "      customer_level_data['chain'].astype('category'))\n",
    "  customer_level_data['dept'] = (customer_level_data['dept'].astype('category'))\n",
    "  customer_level_data['brand'] = (\n",
    "      customer_level_data['brand'].astype('category'))\n",
    "  customer_level_data['category'] = (\n",
    "      customer_level_data['category'].astype('category'))\n",
    "  customer_level_data['label'] = (\n",
    "      customer_level_data['holdout_value'].astype('float32'))\n",
    "  return customer_level_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fP3q6uuMoXhA"
   },
   "source": [
    "### Load customer-level csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8B4zV1xoeMX"
   },
   "outputs": [],
   "source": [
    "def load_customer_level_csv(company):\n",
    "  customer_level_data_file = f'{DATA_FOLDER}/customer_level_data_company_{company}.csv'\n",
    "  if os.path.isfile(customer_level_data_file):\n",
    "    customer_level_data = pd.read_csv(customer_level_data_file)\n",
    "  else:\n",
    "    customer_level_data = preprocess(load_transaction_data(company))\n",
    "  for cat_col in CATEGORICAL_FEATURES:\n",
    "    customer_level_data[cat_col] = (\n",
    "        customer_level_data[cat_col].astype('category'))\n",
    "  for num_col in [\n",
    "      'log_calibration_value', 'calibration_value', 'holdout_value'\n",
    "  ]:\n",
    "    customer_level_data[num_col] = (\n",
    "        customer_level_data[num_col].astype('float32'))\n",
    "\n",
    "  return customer_level_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88dVPdt5QWpu"
   },
   "outputs": [],
   "source": [
    "# Processes data. 350 iteration in total. May take 10min.\n",
    "customer_level_data = load_customer_level_csv(COMPANY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_level_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09tqgvANtsil"
   },
   "source": [
    "We observe a mixture of zero and lognormal distribution of holdout value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtF0z3VbmGev"
   },
   "outputs": [],
   "source": [
    "customer_level_data.label.apply(np.log1p).hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4kN0uk4kZ68"
   },
   "source": [
    "### Make train/eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nc0MLKx2yD72"
   },
   "outputs": [],
   "source": [
    "def linear_split(df):\n",
    "  # get_dummies preserves numeric features.\n",
    "  x = pd.get_dummies(df[ALL_FEATURES], drop_first=True).astype('float32').values\n",
    "  y = df['label'].values\n",
    "  y0 = df['calibration_value'].values\n",
    "\n",
    "  x_train, x_eval, y_train, y_eval, y0_train, y0_eval = (\n",
    "      model_selection.train_test_split(\n",
    "          x, y, y0, test_size=0.2, random_state=123))\n",
    "\n",
    "  return x_train, x_eval, y_train, y_eval, y0_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nc0MLKx2yD72"
   },
   "outputs": [],
   "source": [
    "def linear_split_with_test(df):\n",
    "  # get_dummies preserves numeric features.\n",
    "  x = pd.get_dummies(df[ALL_FEATURES], drop_first=True).astype('float32').values\n",
    "  y = df['label'].values\n",
    "  y0 = df['calibration_value'].values\n",
    "\n",
    "  x_train, x_test, y_train, y_test, y0_train, y0_test = (\n",
    "      model_selection.train_test_split(\n",
    "          x, y, y0, test_size=0.2, random_state=123))\n",
    "\n",
    "  x_train, x_eval, y_train, y_eval, y0_train, y0_eval = (\n",
    "      model_selection.train_test_split(\n",
    "          x_train, y_train, y0_train, test_size=0.2, random_state=123))\n",
    "\n",
    "    \n",
    "  return x_train, x_eval, x_test, y_train, y_eval, y_eval, y0_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAGbXp9ax042"
   },
   "outputs": [],
   "source": [
    "def dnn_split(df):\n",
    "  for key in CATEGORICAL_FEATURES:\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    df[key] = encoder.fit_transform(df[key])\n",
    "\n",
    "  y0 = df['calibration_value'].values\n",
    "  df_train, df_eval, y0_train, y0_eval = model_selection.train_test_split(\n",
    "      df, y0, test_size=0.2, random_state=123)\n",
    "\n",
    "  def feature_dict(df):\n",
    "    features = {k: v.values for k, v in dict(df[CATEGORICAL_FEATURES]).items()}\n",
    "    features['numeric'] = df[NUMERIC_FEATURES].values\n",
    "    return features\n",
    "\n",
    "  x_train, y_train = feature_dict(df_train), df_train['label'].values\n",
    "  x_eval, y_eval = feature_dict(df_eval), df_eval['label'].values\n",
    "\n",
    "  return x_train, x_eval, y_train, y_eval, y0_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAGbXp9ax042"
   },
   "outputs": [],
   "source": [
    "def dnn_split_with_test(df):\n",
    "  for key in CATEGORICAL_FEATURES:\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    df[key] = encoder.fit_transform(df[key])\n",
    "\n",
    "  y0 = df['calibration_value'].values\n",
    "  df_train, df_test, y0_train, y0_test = model_selection.train_test_split(\n",
    "      df, y0, test_size=0.2, random_state=123)\n",
    "\n",
    "  df_train, df_eval, y0_train, y0_eval = model_selection.train_test_split(\n",
    "      df_train, y0_train, test_size=0.2, random_state=123)\n",
    "    \n",
    "  def feature_dict(df):\n",
    "    features = {k: v.values for k, v in dict(df[CATEGORICAL_FEATURES]).items()}\n",
    "    features['numeric'] = df[NUMERIC_FEATURES].values\n",
    "    return features\n",
    "\n",
    "  x_train, y_train = feature_dict(df_train), df_train['label'].values\n",
    "  x_eval, y_eval = feature_dict(df_eval), df_eval['label'].values\n",
    "  x_test, y_test = feature_dict(df_test), df_test['label'].values\n",
    "\n",
    "  return x_train, x_eval, x_test, y_train, y_eval, y_test, y0_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqbShWBzR4NE"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(output_units, input_dim):\n",
    "  return tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(input_dim,)),\n",
    "    tf.keras.layers.Dense(output_units, activation=None)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7huREFbR7Dl"
   },
   "outputs": [],
   "source": [
    "def embedding_dim(x):\n",
    "  return int(x**.25) + 1\n",
    "\n",
    "\n",
    "def embedding_layer(vocab_size):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Embedding(\n",
    "          input_dim=vocab_size,\n",
    "          output_dim=embedding_dim(vocab_size)\n",
    "      ),\n",
    "      tf.keras.layers.Flatten(),\n",
    "  ])\n",
    "\n",
    "\n",
    "def dnn_model(output_units, df):\n",
    "  numeric_input = tf.keras.layers.Input(\n",
    "      shape=(len(NUMERIC_FEATURES),), name='numeric')\n",
    "\n",
    "  embedding_inputs = [\n",
    "      tf.keras.layers.Input(shape=(1,), name=key, dtype=np.int64)\n",
    "      for key in CATEGORICAL_FEATURES\n",
    "  ]\n",
    "\n",
    "  embedding_outputs = [\n",
    "      embedding_layer(vocab_size=df[key].nunique())(input)\n",
    "      for key, input in zip(CATEGORICAL_FEATURES, embedding_inputs)\n",
    "  ]\n",
    "\n",
    "  deep_input = tf.keras.layers.concatenate([numeric_input] + embedding_outputs)\n",
    "  deep_model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(32, activation='relu'),\n",
    "      tf.keras.layers.Dense(output_units),\n",
    "  ])\n",
    "  return tf.keras.Model(\n",
    "      inputs=[numeric_input] + embedding_inputs, outputs=deep_model(deep_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8l-KzZ12fbK"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3HzXsj61uy3"
   },
   "outputs": [],
   "source": [
    "if LOSS == 'mse':\n",
    "  loss = keras.losses.MeanSquaredError()\n",
    "  output_units = 1\n",
    "\n",
    "if LOSS == 'ziln':\n",
    "  loss = ltv.zero_inflated_lognormal_loss\n",
    "  output_units = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pNM4q5m19Dv"
   },
   "outputs": [],
   "source": [
    "if MODEL == 'linear':\n",
    "  x_train, x_eval, x_test, y_train, y_eval, y_test, y0_test = linear_split_with_test(customer_level_data)\n",
    "  model = linear_model(output_units, x_train.shape[1])\n",
    "\n",
    "if MODEL == 'dnn':\n",
    "  x_train, x_eval, x_test, y_train, y_eval, y_test, y0_test = dnn_split_with_test(customer_level_data)\n",
    "  model = dnn_model(output_units, customer_level_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Un-yJPHp31gp"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=loss, optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GQ-RlIAfT62"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', min_lr=1e-6),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BjnHV7MWhK1"
   },
   "outputs": [],
   "source": [
    "# y array needs to have a two dimensional shape to work with ziln loss function\n",
    "# so we use [:, np.newaxis] to make the data two-dimensional for the fit function call\n",
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train[:, np.newaxis],\n",
    "    batch_size=1024,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(x_eval, y_eval[:, np.newaxis])).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAJGs5SebDeN"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history)[['loss', 'val_loss']][2:].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHaiutmy2aYm"
   },
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6E_5gYAYQMw"
   },
   "outputs": [],
   "source": [
    "if LOSS == 'mse':\n",
    "  y_pred = model.predict(x=x_test, batch_size=1024).flatten()\n",
    "\n",
    "if LOSS == 'ziln':\n",
    "  logits = model.predict(x=x_test, batch_size=1024)\n",
    "  y_pred = ltv.zero_inflated_lognormal_pred(logits).numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mm28qKSGXNyr"
   },
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame({\n",
    "    'y_true': y_test,\n",
    "    'y_pred': y_pred,\n",
    "})\n",
    "df_pred.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zROhsEWxnA5u"
   },
   "source": [
    "### Gini Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRsJ7y-632h_"
   },
   "outputs": [],
   "source": [
    "gain = pd.DataFrame({\n",
    "    'lorenz': ltv.cumulative_true(y_test, y_test),\n",
    "    'baseline': ltv.cumulative_true(y_test, y0_test),\n",
    "    'model': ltv.cumulative_true(y_test, y_pred),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yg-ndbve4AL_"
   },
   "outputs": [],
   "source": [
    "num_customers = np.float32(gain.shape[0])\n",
    "gain['cumulative_customer'] = (np.arange(num_customers) + 1.) / num_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEoAvuCj4OVy"
   },
   "outputs": [],
   "source": [
    "ax = gain[[\n",
    "    'cumulative_customer',\n",
    "    'lorenz',\n",
    "    'baseline',\n",
    "    'model',\n",
    "]].plot(\n",
    "    x='cumulative_customer', figsize=(8, 5), legend=True)\n",
    "\n",
    "ax.legend(['Groundtruth', 'Baseline', 'Model'], loc='upper left')\n",
    "\n",
    "ax.set_xlabel('Cumulative Fraction of Customers')\n",
    "ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "ax.set_xlim((0, 1.))\n",
    "\n",
    "ax.set_ylabel('Cumulative Fraction of Total Lifetime Value')\n",
    "ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "ax.set_ylim((0, 1.05))\n",
    "ax.set_title('Gain Chart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzPqaiNO4iWC"
   },
   "outputs": [],
   "source": [
    "gini = ltv.gini_from_gain(gain[['lorenz', 'baseline', 'model']])\n",
    "gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S84RitIa9PBu"
   },
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7sKbsEf6RvF"
   },
   "outputs": [],
   "source": [
    "df_decile = ltv.decile_stats(y_test, y_pred)\n",
    "df_decile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHdLqUqdL4hf"
   },
   "outputs": [],
   "source": [
    "ax = df_decile[['label_mean', 'pred_mean']].plot.bar(rot=0)\n",
    "\n",
    "ax.set_title('Decile Chart')\n",
    "ax.set_xlabel('Prediction bucket')\n",
    "ax.set_ylabel('Average bucket value')\n",
    "ax.legend(['Label', 'Prediction'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nK6DQ89xU-d4"
   },
   "source": [
    "### Rank Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9qWGyY3WePz"
   },
   "outputs": [],
   "source": [
    "def spearmanr(x1: Sequence[float], x2: Sequence[float]) -> float:\n",
    "  \"\"\"Calculates spearmanr rank correlation coefficient.\n",
    "\n",
    "  See https://docs.scipy.org/doc/scipy/reference/stats.html.\n",
    "\n",
    "  Args:\n",
    "    x1: 1D array_like.\n",
    "    x2: 1D array_like.\n",
    "\n",
    "  Returns:\n",
    "    correlation: float.\n",
    "  \"\"\"\n",
    "  return stats.spearmanr(x1, x2, nan_policy='raise')[0]\n",
    "\n",
    "\n",
    "spearman_corr = spearmanr(y_test, y_pred)\n",
    "spearman_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-i_AbqhXcurk"
   },
   "source": [
    "### All metrics together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Umqg1-0Bc1HS"
   },
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame(\n",
    "    {\n",
    "        'company': COMPANY,\n",
    "        'model': MODEL,\n",
    "        'loss': LOSS,\n",
    "        'label_mean': y_test.mean(),\n",
    "        'pred_mean': y_pred.mean(),\n",
    "        'label_positive': np.mean(y_test > 0),\n",
    "        'decile_mape': df_decile['decile_mape'].mean(),\n",
    "        'baseline_gini': gini['normalized'][1],\n",
    "        'gini': gini['normalized'][2],\n",
    "        'spearman_corr': spearman_corr,\n",
    "    },\n",
    "    index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LV1Hs3xcxnd"
   },
   "outputs": [],
   "source": [
    "df_metrics[[\n",
    "    'company',\n",
    "    'model',\n",
    "    'loss',\n",
    "    'label_mean',\n",
    "    'pred_mean',\n",
    "    'label_positive',\n",
    "    'decile_mape',\n",
    "    'baseline_gini',\n",
    "    'gini',\n",
    "    'spearman_corr',\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVy6lYn4mSrj"
   },
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtkQ4mqUEFsb"
   },
   "outputs": [],
   "source": [
    "output_path = os.path.join(OUTPUT_CSV_FOLDER, COMPANY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qmLzJqOEFsm"
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(output_path):\n",
    "  os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61B5Zc_UEFsr"
   },
   "outputs": [],
   "source": [
    "output_file = os.path.join(output_path,\n",
    "                           '{}_regression_{}.csv'.format(MODEL, LOSS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqglbXfwEFsv"
   },
   "outputs": [],
   "source": [
    "df_metrics.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "name": "regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
